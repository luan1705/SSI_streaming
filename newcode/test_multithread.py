import orjson
import config
import asyncio
import threading
import time
from datetime import datetime, time as dtime, date
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
import logging
import random
from exchange_map import exchange_map
from indices_map import indices_map

from ssi_fc_data.fc_md_stream import MarketDataStream
from ssi_fc_data.fc_md_client import MarketDataClient

# DB
import pandas as pd
from pandas import json_normalize
from sqlalchemy import create_engine, MetaData, Table, Column, String, Float, Integer
from sqlalchemy.dialects.postgresql import insert as pg_insert
from sqlalchemy.exc import OperationalError

# ==================== Optional symbol lists (ƒë·ªÉ batch) ====================
try:
    from symbols_list import symbols_list as SYMBOLS_X
except Exception:
    SYMBOLS_X = None

try:
    from symbols_list import symbols_list as SYMBOLS_R
except Exception:
    SYMBOLS_R = None

try:
    from mi_indices_list import mi_indices_list as INDICES_MI
except Exception:
    INDICES_MI = None

# ==================== Logging ====================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler("stream.log"), logging.StreamHandler()],
)

# ==================== FastAPI ====================
app = FastAPI(title="Streaming WebSocket + Multi-DB Production")

clients = {"X": set(), "R": set(), "MI": set()}
last_msg_time = {"X": None, "R": None, "MI": None}
holiday = [date(2026, 1, 1)]

# ==================== Concurrency Guards ====================
# Gi·ªõi h·∫°n s·ªë k·∫øt n·ªëi WS ho·∫°t ƒë·ªông c√πng l√∫c (ƒëi·ªÅu ch·ªânh theo t√†i nguy√™n m√°y)
# MAX_ACTIVE_CONN = 1000
# GLOBAL_CONN_SEM = threading.Semaphore(MAX_ACTIVE_CONN)

# Gi·ªõi h·∫°n s·ªë lu·ªìng g·ªçi AccessToken ƒë·ªìng th·ªùi (gi·∫£m b√£o token)
TOKEN_FETCH_SEM = threading.Semaphore(1)

# ==================== DB setup ====================
engine = create_engine(
    "postgresql+psycopg2://vnsfintech:%40Vns123456@videv.cloud:5432/vnsfintech",
    echo=False,
    pool_pre_ping=True,
    pool_size=10,       # gi·ªõi h·∫°n pool
    max_overflow=20,    # slot t·∫°m
    pool_recycle=1800,  # t√°i ch·∫ø k·∫øt n·ªëi l√¢u ng√†y
)
metadata = MetaData()

x_table = Table(
    "eboard_table", metadata,
    Column('symbol', String, primary_key=True),
    Column('exchange', String),
    Column('indices', String),
    Column('ceiling', Float),
    Column('floor', Float),
    Column('refPrice', Float),
    Column('buyPrice3', Float), Column('buyVol3', Float),
    Column('buyPrice2', Float), Column('buyVol2', Float),
    Column('buyPrice1', Float), Column('buyVol1', Float),
    Column('matchPrice', Float), Column('matchVol', Float),
    Column('matchChange', Float), Column('matchRatioChange', Float),
    Column('sellPrice1', Float), Column('sellVol1', Float),
    Column('sellPrice2', Float), Column('sellVol2', Float),
    Column('sellPrice3', Float), Column('sellVol3', Float),
    Column('totalVol', Float), Column('totalVal', Float),
    Column('high', Float), Column('low', Float), Column('open', Float), Column('close', Float),
    schema="history_data"
)

r_table = Table(
    "eboard_foreign", metadata,
    Column("symbol", String, primary_key=True),
    Column("buyVol", Float),
    Column("sellVol", Float),
    Column("room", Float),
    Column("buyVal", Float),
    Column("sellVal", Float),
    schema="history_data"
)

mi_table = Table(
    "indices", metadata,
    Column("symbol", String, primary_key=True),
    Column("point", Float),
    Column("change", Float),
    Column("ratioChange", Float),
    Column("totalVol", Float),
    Column("totalVal", Float),
    Column("advancers", Integer),
    Column("noChange", Integer),
    Column("decliners", Integer),
    schema="history_data"
)

metadata.create_all(engine)

# ==================== Trading time check ====================
def is_trading_time():
    now = datetime.now()
    today = now.date()
    if now.weekday() >= 5 or today in holiday:
        return False
    t9h = dtime(9, 0); t12h = dtime(12, 0)
    t13h = dtime(13, 0); t15h = dtime(15, 0)
    return (t9h <= now.time() <= t12h) or (t13h <= now.time() <= t15h)

# ==================== WebSocket broadcast ====================
async def broadcast(channel, data: dict):
    dead_clients = []
    for ws in list(clients[channel]):
        try:
            await ws.send_text(orjson.dumps(data).decode())
        except Exception:
            dead_clients.append(ws)
    for ws in dead_clients:
        clients[channel].discard(ws)

def schedule_broadcast(channel: str, data: dict):
    """G·ªçi t·ª´ thread an to√†n: ch·ªâ schedule n·∫øu loop ƒëang ch·∫°y."""
    loop = getattr(app.state, "loop", None)
    if loop is None or loop.is_closed() or not loop.is_running():
        logging.warning("üî∏ Loop ch∆∞a s·∫µn s√†ng/ƒë√£ ƒë√≥ng ‚Üí drop broadcast %s", channel)
        return
    try:
        asyncio.run_coroutine_threadsafe(broadcast(channel, data), loop)
    except Exception:
        logging.exception("üî∏ schedule_broadcast(%s) l·ªói", channel)

async def websocket_endpoint(websocket: WebSocket, channel: str):
    await websocket.accept()
    clients[channel].add(websocket)
    logging.info(f"‚úÖ Client connected: {websocket.client} to {channel}")
    try:
        while True:
            await websocket.receive_text()
    except WebSocketDisconnect:
        logging.info(f"‚ùå Client disconnected: {websocket.client} from {channel}")
    finally:
        clients[channel].discard(websocket)

@app.websocket("/ws/eboard_table")
async def websocket_x(ws: WebSocket):
    await websocket_endpoint(ws, "X")

@app.websocket("/ws/eboard_foreign")
async def websocket_r(ws: WebSocket):
    await websocket_endpoint(ws, "R")

@app.websocket("/ws/indices")
async def websocket_mi(ws: WebSocket):
    await websocket_endpoint(ws, "MI")

@app.on_event("shutdown")
async def shutdown_event():
    for ch in clients:
        clients[ch].clear()
    logging.info("üßπ Shutdown: cleared clients")

# ==================== DB retry helper ====================
def db_upsert_with_retry(do_upsert_func, label: str, max_tries=5, base_delay=0.5):
    delay = base_delay
    for attempt in range(1, max_tries+1):
        try:
            do_upsert_func()
            return True
        except OperationalError as e:
            logging.error(f"‚ùå {label} DB op failed (attempt {attempt}/{max_tries}): {e}")
            time.sleep(delay)
            delay = min(delay * 2, 5.0)
        except Exception as e:
            logging.exception(f"üí• {label} unexpected DB error")
            return False
    logging.error(f"‚õî {label} DB op giving up after {max_tries} attempts")
    return False

# ==================== DB upsert functions ====================
def save_x(result):
    try:
        c = result["content"]
        indices = c.get("indices")
        if isinstance(indices, list):
            indices = "|".join(indices)
        bp = c["buy"]["price"]; bv = c["buy"]["vol"]
        sp = c["sell"]["price"]; sv = c["sell"]["vol"]
        m = c["match"]

        row = {
            "symbol":  c["symbol"],
            "exchange": c.get("exchange"),
            "indices":  indices,
            "ceiling":  c["ceiling"], "floor": c["floor"],
            "refPrice": c["refPrice"],
            "buyPrice1": bp[0], "buyVol1": bv[0],
            "buyPrice2": bp[1], "buyVol2": bv[1],
            "buyPrice3": bp[2], "buyVol3": bv[2],
            "matchPrice": m["price"], "matchVol": m["vol"],
            "matchChange": m["change"], "matchRatioChange": m["ratioChange"],
            "sellPrice1": sp[0], "sellVol1": sv[0],
            "sellPrice2": sp[1], "sellVol2": sv[1],
            "sellPrice3": sp[2], "sellVol3": sv[2],
            "totalVol": c["totalVol"], "totalVal": c.get("totalVal"),
            "high": c["high"], "low": c["low"],
            "open": c.get("open"), "close": c.get("close"),
        }

        def _do():
            with engine.begin() as conn:
                stmt = pg_insert(x_table).values([row])
                update_dict = {k: getattr(stmt.excluded, k) for k in row if k != "symbol"}
                stmt = stmt.on_conflict_do_update(index_elements=["symbol"], set_=update_dict)
                conn.execute(stmt)

        if db_upsert_with_retry(_do, label=f"X:{c.get('symbol','')}"):
            logging.info("‚úÖ X upserted %s", c.get("symbol",""))

    except Exception as e:
        logging.error("‚ùå X DB save error: %s", e)

def save_r(result):
    try:
        c = result["content"]
        row = {
            "symbol": c["symbol"], "buyVol": c["buyVol"], "sellVol": c["sellVol"],
            "room": c["room"], "buyVal": c["buyVal"], "sellVal": c["sellVal"],
        }

        def _do():
            with engine.begin() as conn:
                stmt = pg_insert(r_table).values([row])
                update_dict = {k: getattr(stmt.excluded, k) for k in row if k != "symbol"}
                stmt = stmt.on_conflict_do_update(index_elements=["symbol"], set_=update_dict)
                conn.execute(stmt)

        if db_upsert_with_retry(_do, label=f"R:{c.get('symbol','')}"):
            logging.info("‚úÖ R upserted %s", c.get("symbol",""))

    except Exception as e:
        logging.error("‚ùå R DB save error: %s", e)

def save_mi(result):
    try:
        c = result["content"]
        raw = c.get("advancersDecliners")
        if isinstance(raw, dict):
            seq = [raw.get("Advances"), raw.get("NoChanges"), raw.get("Declines")]
        else:
            seq = list(raw) if raw is not None else []
        adv, nc, dec = (seq + [None, None, None])[:3]
        row = {
            "symbol": c["symbol"], "point": c["point"], "change": c["change"],
            "ratioChange": c["ratioChange"], "totalVol": c["totalVol"], "totalVal": c["totalVal"],
            "advancers": adv, "noChange": nc, "decliners": dec,
        }

        def _do():
            with engine.begin() as conn:
                stmt = pg_insert(mi_table).values([row])
                update_dict = {k: getattr(stmt.excluded, k) for k in row if k != "symbol"}
                stmt = stmt.on_conflict_do_update(index_elements=["symbol"], set_=update_dict)
                conn.execute(stmt)

        if db_upsert_with_retry(_do, label=f"MI:{c.get('symbol','')}"):
            logging.info("‚úÖ MI upserted %s", c.get("symbol",""))

    except Exception as e:
        logging.error("‚ùå MI DB save error: %s", e)

# ==================== Helpers ====================
def find_exchange(symbol: str, exchange_map: dict):
    for exchange, symbols in exchange_map.items():
        if symbol in symbols:
            return exchange
    return None

def find_indice(symbol: str, indices_map: dict):
    indices_list=[]
    for indices, symbols in indices_map.items():
        if symbol in symbols:
            indices_list.append(indices)
    return indices_list if indices_list else None

def _k(x):
    if x is None: return None
    return x/1000

def _safe_ratio(last_price, ref_price):
    if last_price is None or ref_price in (None,0): return None
    return (last_price-ref_price)/ref_price*100.0

# ==================== Message handlers ====================
def on_message_X(message):
    global last_msg_time
    try:
        data = orjson.loads(message.get("Content","{}"))
        symbol = data['Symbol']
        exchange = find_exchange(symbol, exchange_map)
        indices = find_indice(symbol, indices_map)
        result = {
            'function':'eboard_table',
            'content': {
                'symbol': symbol,'exchange': exchange,'indices': indices,
                'ceiling': _k(data.get('Ceiling')),'floor': _k(data.get('Floor')),
                'refPrice': _k(data.get('RefPrice')),
                'buy':{'price': [_k(data.get('BidPrice1')),_k(data.get('BidPrice2')),_k(data.get('BidPrice3'))],
                       'vol': [data.get('BidVol1'), data.get('BidVol2'), data.get('BidVol3')]},
                'match':{'price': _k(data.get('LastPrice')),'vol': data.get('LastVol'),
                         'change': _k(data.get('Change')),
                         'ratioChange': _safe_ratio(_k(data.get('LastPrice')), _k(data.get('RefPrice')))},
                'sell':{'price': [_k(data.get('AskPrice1')),_k(data.get('AskPrice2')),_k(data.get('AskPrice3'))],
                        'vol': [data.get('AskVol1'), data.get('AskVol2'), data.get('AskVol3')]},
                'totalVol': data.get('TotalVol'),'totalVal': data.get('TotalVal'),
                'high': _k(data.get('High')),'low': _k(data.get('Low')),
                'open': _k(data.get('Open')),'close': _k(data.get('Close')),
            }
        }
        schedule_broadcast("X", result)
        save_x(result); last_msg_time["X"] = time.time()
    except Exception:
        logging.exception("‚ùó X message error")

def on_message_R(message):
    global last_msg_time
    try:
        data = orjson.loads(message.get("Content","{}"))
        symbol = data['Symbol']
        result = {'function': 'eboard_foreign','content': {
            'symbol': symbol,'buyVol': data.get('BuyVol'),'sellVol': data.get('SellVol'),
            'room': data.get('CurrentRoom'),'buyVal': data.get('BuyVal'),'sellVal': data.get('SellVal')}}
        schedule_broadcast("R", result)
        save_r(result); last_msg_time["R"] = time.time()
    except Exception:
        logging.exception("‚ùó R message error")

def on_message_MI(message):
    global last_msg_time
    try:
        data = orjson.loads(message.get("Content","{}"))
        symbol = data.get('IndexId')
        if symbol == 'HNXUpcomIndex': symbol = 'UPCOMINDEX'
        elif symbol == 'HNXIndex': symbol = 'HNXINDEX'
        result = {'function': 'indices','content': {
            'symbol': symbol,'point': data.get('IndexValue'),'change': data.get('Change'),
            'ratioChange': data.get('RatioChange'),'totalVol': data.get('AllQty'),'totalVal': data.get('AllValue'),
            'advancersDecliners': [(data.get('Advances') or 0)+(data.get('Ceilings') or 0),
                                   data.get('NoChanges'),
                                   (data.get('Declines') or 0)+(data.get('Floors') or 0)]}}
        schedule_broadcast("MI", result)
        save_mi(result); last_msg_time["MI"] = time.time()
    except Exception:
        logging.exception("‚ùó MI message error")

# ==================== Stream supervisor (auto-reconnect + token throttling) ====================
def on_error(error, channel_name=None):
    if channel_name:
        logging.error(f"‚ùó [{channel_name}] WebSocket error: {error}")
    else:
        logging.error(f"‚ùó WebSocket error: {error}")

def stream_supervisor(channel_name, on_message_func, stream_code, open_hours_only=False):
    backoff = 1.0
    backoff_max = 30.0
    jitter_s = (0.0, 1.0)
    while True:
        try:
            if open_hours_only and not is_trading_time():
                time.sleep(60)
                continue

            # ----- h·∫°n ch·∫ø ƒë·ªìng th·ªùi khi xin token -----
            client = None
            for attempt in range(1, 6):  # th·ª≠ t·ªëi ƒëa 5 l·∫ßn
                try:
                    acquired = TOKEN_FETCH_SEM.acquire(timeout=10)
                    if not acquired:
                        raise RuntimeError("Token semaphore timeout")
                    try:
                        # r·∫£i ƒë·ªÅu y√™u c·∫ßu token m·ªôt ch√∫t
                        time.sleep(random.uniform(0.0, 0.3))
                        client = MarketDataClient(config)     # g·ªçi /api/v2/Market/AccessToken
                        break  # OK
                    finally:
                        TOKEN_FETCH_SEM.release()
                except Exception as e:
                    sleep_s = min(backoff, backoff_max) + random.uniform(*jitter_s)
                    logging.error(f"üîê [{channel_name}] get token fail (try {attempt}/5): {e}. Retry in {sleep_s:.1f}s")
                    time.sleep(sleep_s)
                    backoff = min(backoff * 2, backoff_max)

            if client is None:
                # b·ªè qua v√≤ng n√†y, ti·∫øp t·ª•c v√≤ng while ƒë·ªÉ th·ª≠ l·∫°i
                continue

            mm = MarketDataStream(config, client)
            done_evt = threading.Event()

            def _run_once():
                try:
                    logging.info(f"üöÄ [{channel_name}] Connecting {stream_code} ...")
                    mm.start(on_message_func, lambda e: on_error(e, channel_name), stream_code)
                except Exception as e:
                    logging.exception(f"üí• [{channel_name}] mm.start() exception: {e}")
                finally:
                    done_evt.set()
                    logging.warning(f"üîå [{channel_name}] stream ended")

            t = threading.Thread(target=_run_once, name=f"{channel_name}-ws", daemon=True)
            t.start()

            while not done_evt.is_set():
                time.sleep(0.2)

            sleep_s = min(backoff, backoff_max) + random.uniform(*jitter_s)
            logging.info(f"üîÅ [{channel_name}] Reconnecting in {sleep_s:.1f}s ...")
            time.sleep(sleep_s)
            backoff = min(backoff * 2, backoff_max)

        except Exception as e:
            logging.exception(f"‚ö†Ô∏è [{channel_name}] supervisor loop exception: {e}")
            sleep_s = min(backoff, backoff_max) + random.uniform(*jitter_s)
            time.sleep(sleep_s)
            backoff = min(backoff * 2, backoff_max)

def start_stream(channel_name, on_message_func, stream_code, open_hours_only=False, delay_first_connect_s=0.0):
    def _target():
        if delay_first_connect_s > 0:
            time.sleep(delay_first_connect_s)
        # GLOBAL_CONN_SEM.acquire()
        # try:
            stream_supervisor(channel_name, on_message_func, stream_code, open_hours_only=open_hours_only)
        # finally:
            # GLOBAL_CONN_SEM.release()
    threading.Thread(target=_target, name=f"{channel_name}-supervisor", daemon=True).start()

# ==================== Batching helpers ====================
def batch_symbols(symbols, batch_size: int, prefix: str):
    """
    Tr·∫£ v·ªÅ list c√°c tuple (group_list, stream_code)
    stream_code d·∫°ng: f"{prefix}" + "-".join(group)
    V√≠ d·ª•: (["ACB","VCB","HPG"], "X:ACB-VCB-HPG")
    """
    batches = []
    for i in range(0, len(symbols or []), batch_size):
        group = symbols[i:i+batch_size]
        code = prefix + "-".join(group)
        batches.append((group, code))
    return batches

# ==================== FastAPI startup (batched per channel) ====================
@app.on_event("startup")
async def startup_event():
    loop = asyncio.get_running_loop()
    app.state.loop = loop

    # Tu·ª≥ ch·ªânh k√≠ch th∆∞·ªõc batch & nh·ªãp m·ªü k·∫øt n·ªëi (stagger)
    X_BATCH_SIZE = 50     # s·ªë m√£ / 1 k·∫øt n·ªëi X (gi·∫£m t·ªïng k·∫øt n·ªëi)
    R_BATCH_SIZE = 50     # s·ªë m√£ / 1 k·∫øt n·ªëi R
    MI_BATCH_SIZE = 5     # s·ªë ch·ªâ s·ªë / 1 k·∫øt n·ªëi MI
    STAGGER = 0.4          # gi√£n c√°ch m·ªü k·∫øt n·ªëi ~5 conn/gi√¢y

    # ===== X: batch th√†nh "X:ACB-VCB-HPG-..." =====
    if SYMBOLS_X and isinstance(SYMBOLS_X, (list, tuple)) and len(SYMBOLS_X) > 0:
        x_batches = batch_symbols(SYMBOLS_X, X_BATCH_SIZE, prefix="X:")
        for i, (_group, code) in enumerate(x_batches, 1):
            start_stream(
                channel_name=f"X-BATCH-{i:02d}",
                on_message_func=on_message_X,
                stream_code=code,
                open_hours_only=False,
                delay_first_connect_s=STAGGER * (i-1),
            )
        logging.info(f"‚úÖ X batched connections: {len(x_batches)} (batch size={X_BATCH_SIZE})")
    else:
        logging.warning("‚ö†Ô∏è SYMBOLS_X tr·ªëng/kh√¥ng h·ª£p l·ªá ‚Üí kh√¥ng m·ªü k·∫øt n·ªëi X.")

    # ===== R: batch th√†nh "R:ACB-VCB-HPG-..." =====
    if SYMBOLS_R and isinstance(SYMBOLS_R, (list, tuple)) and len(SYMBOLS_R) > 0:
        r_batches = batch_symbols(SYMBOLS_R, R_BATCH_SIZE, prefix="R:")
        for i, (_group, code) in enumerate(r_batches, 1):
            start_stream(
                channel_name=f"R-BATCH-{i:02d}",
                on_message_func=on_message_R,
                stream_code=code,
                open_hours_only=False,
                delay_first_connect_s=STAGGER * (i-1),
            )
        logging.info(f"‚úÖ R batched connections: {len(r_batches)} (batch size={R_BATCH_SIZE})")
    else:
        logging.warning("‚ö†Ô∏è SYMBOLS_R tr·ªëng/kh√¥ng h·ª£p l·ªá ‚Üí kh√¥ng m·ªü k·∫øt n·ªëi R.")

    # ===== MI: batch th√†nh "MI:VNINDEX-VN30-..." =====
    if INDICES_MI and isinstance(INDICES_MI, (list, tuple)) and len(INDICES_MI) > 0:
        mi_batches = batch_symbols(INDICES_MI, MI_BATCH_SIZE, prefix="MI:")
        for i, (_group, code) in enumerate(mi_batches, 1):
            start_stream(
                channel_name=f"MI-BATCH-{i:02d}",
                on_message_func=on_message_MI,
                stream_code=code,
                open_hours_only=False,
                delay_first_connect_s=STAGGER * (i-1),
            )
        logging.info(f"‚úÖ MI batched connections: {len(mi_batches)} (batch size={MI_BATCH_SIZE})")
    else:
        logging.warning("‚ö†Ô∏è INDICES_MI tr·ªëng/kh√¥ng h·ª£p l·ªá ‚Üí kh√¥ng m·ªü k·∫øt n·ªëi MI.")

    logging.info("üöÄ Batched per-channel streaming + WebSocket + DB services started")
